# LoRA/PEFT fine-tuning for NLLB (fast-mode)
# Medical domain EN->HI with glossary bias
# Optimized for MX550 (2GB VRAM) and small dataset (2k examples)
# Faster training (~4â€“5 hours)
training:
  type: "lora"
  output_dir: "./models/custom-nllb-fast"
  save_strategy: "no"   # don't save checkpoints for fast test

  data:
    eval_split: 0.05   # MUST be > 0 and < 1 (5% eval)

  hyperparameters:
    num_train_epochs: 1
    per_device_train_batch_size: 1
    per_device_eval_batch_size: 1
    learning_rate: 2.0e-4
    gradient_accumulation_steps: 16

  optimization:
    fp16: false
    bf16: false
    gradient_checkpointing: false
    optim: "adamw_torch"

  logging:
    strategy: "steps"
    steps: 20

  evaluation:
    strategy: "no"

lora:
  r: 8
  lora_alpha: 16
  lora_dropout: 0.05
  target_modules:
    - "q_proj"
    - "v_proj"
  bias: "none"
  task_type: "SEQ_2_SEQ_LM"
  inference_mode: false

preprocessing:
  max_source_length: 64

seed: 42
