# Production-Ready NLLB Translation Service Architecture

## ğŸ“ Enhanced Folder Structure

```
nllb-translation-service/
â”‚
â”œâ”€â”€ app/                                # Main application
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                         # FastAPI app with lifespan
â”‚   â”‚
â”‚   â”œâ”€â”€ api/                            # API layer
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ routes.py                   # API endpoints
â”‚   â”‚   â”œâ”€â”€ middleware.py               # Custom middleware
â”‚   â”‚   â””â”€â”€ dependencies.py             # FastAPI dependencies
â”‚   â”‚
â”‚   â”œâ”€â”€ core/                           # Core configuration
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ config.py                   # Settings management
â”‚   â”‚   â”œâ”€â”€ logging_config.py           # Structured logging
â”‚   â”‚   â””â”€â”€ metrics.py                  # NEW: Prometheus metrics
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                         # Data models
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ schemas.py                  # Pydantic models
â”‚   â”‚
â”‚   â”œâ”€â”€ services/                       # Business logic
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ translator.py               # Enhanced translation service
â”‚   â”‚   â”œâ”€â”€ language_codes.py           # Language validation
â”‚   â”‚   â””â”€â”€ model_manager.py            # NEW: Model lifecycle
â”‚   â”‚
â”‚   â””â”€â”€ glossary/                       # Glossary processing
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ processor.py
â”‚
â”œâ”€â”€ training/                           # Training pipeline
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ train.py                        # Full fine-tuning
â”‚   â”œâ”€â”€ train_lora.py                   # NEW: LoRA/PEFT training
â”‚   â”œâ”€â”€ evaluate.py                     # Model evaluation
â”‚   â”œâ”€â”€ export_onnx.py                  # ONNX export
â”‚   â””â”€â”€ data_loader.py                  # NEW: Dataset utilities
â”‚
â”œâ”€â”€ inference/                          # NEW: Inference optimizations
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ batch_processor.py              # Parallel batch processing
â”‚   â”œâ”€â”€ cache_manager.py                # Translation caching
â”‚   â””â”€â”€ warmup.py                       # Model warmup utilities
â”‚
â”œâ”€â”€ configs/                            # NEW: Configuration files
â”‚   â”œâ”€â”€ model_config.yaml               # Model settings
â”‚   â”œâ”€â”€ training_config.yaml            # Training hyperparameters
â”‚   â”œâ”€â”€ inference_config.yaml           # Inference settings
â”‚   â””â”€â”€ logging_config.yaml             # Logging configuration
â”‚
â”œâ”€â”€ scripts/                            # Utility scripts
â”‚   â”œâ”€â”€ download_model.py               # Model downloader
â”‚   â”œâ”€â”€ setup_mac.sh                    # NEW: MacBook setup script
â”‚   â”œâ”€â”€ benchmark.py                    # NEW: Performance benchmarks
â”‚   â””â”€â”€ convert_to_lora.py              # NEW: Convert full model to LoRA
â”‚
â”œâ”€â”€ tests/                              # Test suite
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py
â”‚   â”œâ”€â”€ test_api.py
â”‚   â”œâ”€â”€ test_translation.py
â”‚   â”œâ”€â”€ test_lora.py                    # NEW: LoRA tests
â”‚   â””â”€â”€ test_batch_processing.py        # NEW: Batch tests
â”‚
â”œâ”€â”€ data/                               # Data files
â”‚   â”œâ”€â”€ example_glossary_medical.json
â”‚   â””â”€â”€ training/                       # Training datasets
â”‚       â””â”€â”€ .gitkeep
â”‚
â”œâ”€â”€ models/                             # Model storage
â”‚   â”œâ”€â”€ cache/                          # HF cache
â”‚   â”œâ”€â”€ custom-nllb/                    # Fine-tuned models
â”‚   â””â”€â”€ lora-adapters/                  # NEW: LoRA adapters
â”‚
â”œâ”€â”€ monitoring/                         # NEW: Monitoring configs
â”‚   â”œâ”€â”€ prometheus.yml                  # Prometheus config
â”‚   â””â”€â”€ grafana/                        # Grafana dashboards
â”‚       â””â”€â”€ translation_dashboard.json
â”‚
â”œâ”€â”€ docs/                               # NEW: Documentation
â”‚   â”œâ”€â”€ API.md                          # API documentation
â”‚   â”œâ”€â”€ DEPLOYMENT.md                   # Deployment guide
â”‚   â””â”€â”€ TRAINING.md                     # Training guide
â”‚
â”œâ”€â”€ .env.example                        # Environment variables template
â”œâ”€â”€ .env                                # Local environment (gitignored)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ Dockerfile                          # GPU-enabled Docker
â”œâ”€â”€ docker-compose.yml                  # Complete stack
â”œâ”€â”€ docker-compose.mac.yml              # NEW: Mac-specific compose
â”œâ”€â”€ requirements.txt                    # Core dependencies
â”œâ”€â”€ requirements-dev.txt                # Development dependencies
â”œâ”€â”€ requirements-training.txt           # NEW: Training-specific deps
â”œâ”€â”€ Makefile                            # Build automation
â”œâ”€â”€ pytest.ini                          # Test configuration
â”œâ”€â”€ README.md                           # Main documentation
â””â”€â”€ QUICKSTART.md                       # Quick start guide
```

## ğŸ¯ Key Improvements

### 1. **Configuration Management**
- YAML-based configs for different environments
- Centralized settings with validation
- Environment-specific overrides

### 2. **Training Enhancements**
- **LoRA/PEFT Support**: Memory-efficient fine-tuning
- **Training Configs**: YAML-based hyperparameter management
- **Data Loading**: Optimized dataset utilities

### 3. **Inference Optimizations**
- **Parallel Batch Processing**: Process multiple translations concurrently
- **Translation Caching**: Redis/in-memory caching
- **Model Warmup**: Reduce cold start latency
- **Dynamic Batching**: Group requests for efficiency

### 4. **Monitoring & Observability**
- Prometheus metrics (latency, throughput, errors)
- Structured logging with correlation IDs
- Health checks with detailed status
- Performance benchmarking tools

### 5. **Production Features**
- Graceful shutdown handling
- Request timeout management
- Circuit breaker pattern
- Retry logic with exponential backoff
- Rate limiting per endpoint

### 6. **MacBook Development**
- MPS (Metal Performance Shaders) support for M1/M2/M3
- CPU-optimized inference
- Development docker-compose
- Setup automation scripts

## ğŸ”§ Technology Stack

| Component | Technology |
|-----------|-----------|
| **Framework** | FastAPI 0.109+ |
| **ML Framework** | PyTorch 2.1+, Transformers 4.36+ |
| **Efficient Training** | PEFT (LoRA, QLoRA) |
| **Model** | facebook/nllb-200-distilled-600M |
| **Inference** | FP16, torch.compile (PyTorch 2.0+) |
| **API Server** | Uvicorn + Gunicorn |
| **Monitoring** | Prometheus + Grafana |
| **Caching** | Redis (optional) |
| **Testing** | pytest, pytest-asyncio |
| **Logging** | python-json-logger |

## ğŸš€ Architecture Decisions

### Why LoRA/PEFT?
- **Memory Efficiency**: Fine-tune with 3-4x less GPU memory
- **Speed**: Faster training iterations
- **Storage**: Adapter files are 10-100MB vs full models (1GB+)
- **Flexibility**: Multiple adapters for different domains

### Why Separate Inference Module?
- **Separation of Concerns**: Training vs inference logic
- **Optimization**: Dedicated batch processing
- **Caching**: Centralized cache management
- **Testability**: Easier unit testing

### Why YAML Configs?
- **Version Control**: Track config changes
- **Reproducibility**: Exact training runs
- **Multi-Environment**: Dev, staging, production configs
- **Documentation**: Self-documenting configurations

## ğŸ“Š Performance Targets

| Metric | Target | Achieved With |
|--------|--------|---------------|
| **Latency (p50)** | < 200ms | FP16, model warmup |
| **Latency (p99)** | < 500ms | Batch processing |
| **Throughput** | 100+ req/s | Parallel processing |
| **GPU Memory** | < 2GB | FP16 inference |
| **Training Time** | 50% faster | LoRA vs full fine-tuning |

## ğŸ” Security Best Practices

1. **Input Validation**: Pydantic models with strict validation
2. **Rate Limiting**: Per-IP and per-endpoint limits
3. **Timeout Protection**: Request and model inference timeouts
4. **CORS**: Configurable allowed origins
5. **Health Checks**: Authenticated health endpoints for sensitive data
6. **Secrets Management**: Environment variables, no hardcoded secrets

## ğŸ“ˆ Scalability Strategy

1. **Horizontal Scaling**: Stateless API design
2. **Load Balancing**: Kubernetes/Docker Swarm ready
3. **Model Replication**: Shared model cache via NFS/S3
4. **Async Processing**: Background task queues for large batches
5. **Caching Layer**: Redis for repeated translations

## ğŸ§ª Testing Strategy

- **Unit Tests**: Individual components
- **Integration Tests**: API endpoints
- **Load Tests**: Locust/k6 for performance
- **Model Tests**: Translation quality benchmarks
- **CI/CD**: GitHub Actions for automated testing

## ğŸ“ Training Workflows

### Full Fine-Tuning
```bash
python training/train.py \
  --data-file data/training/medical.tsv \
  --source-lang eng_Latn \
  --target-lang hin_Deva \
  --config configs/training_config.yaml
```

### LoRA Fine-Tuning
```bash
python training/train_lora.py \
  --data-file data/training/medical.tsv \
  --source-lang eng_Latn \
  --target-lang hin_Deva \
  --lora-r 16 \
  --lora-alpha 32 \
  --config configs/training_config.yaml
```

## ğŸ MacBook Optimization

### Apple Silicon (M1/M2/M3)
- Use MPS backend: `device = torch.device("mps")`
- Optimize for ARM architecture
- Leverage unified memory

### Intel MacBook
- CPU-optimized inference
- Use smaller batch sizes
- Consider ONNX runtime for speed

## ğŸ“¦ Deployment Options

1. **Docker**: Single container deployment
2. **Docker Compose**: Full stack (API + Monitoring)
3. **Kubernetes**: Production-grade orchestration
4. **Cloud Platforms**: AWS SageMaker, GCP AI Platform, Azure ML
5. **Serverless**: AWS Lambda (with cold start mitigation)

---

**Next Steps**: Implementing the enhanced modules...
